{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG-02 Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's demonstration will introduce epoching and event-related potential analysis using `mne-python`. We will inspect EEG data in response to visual and auditory stimuli. We will start by loading the data from last session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.io import read_raw_fif\n",
    "\n",
    "## Load data.\n",
    "f = os.path.join('..','data','sub-01_task-audvis_preproc_raw.fif')\n",
    "raw = read_raw_fif(f, preload=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Finding and defining events\n",
    "\n",
    "In addition to the EEG and peripheral channels, our recording includes trigger channels. Trigger channels mark the onset/offset of events during recording. In our recording in particular, STI 014 is the trigger channel that was used for combining all the events to a single channel. It has several pulses of different amplitude throughout the recording. These pulses correspond to different stimuli presented to the subject during the acquisition. The pulses and their corresponding events are defined in the table below.\n",
    "\n",
    "| Name   | #  | Contents                                |\n",
    "|--------|----|-----------------------------------------|\n",
    "| LA     | 1  | Response to left-ear auditory stimulus  |\n",
    "| RA     | 2  | Response to right-ear auditory stimulus |\n",
    "| LV     | 3  | Response to left visual field stimulus  |\n",
    "| RV     | 4  | Response to right visual field stimulus |\n",
    "| Smiley | 5  | Response to the smiley face             |\n",
    "| Button | 32 | Response triggered by the button press  |\n",
    "\n",
    "These are the events we are going to align the epochs to. To create an event list from raw data, we simply call a function dedicated just for that. Since the event list is simply a numpy array, you can also manually create one. If you create one from an outside source (like a separate file of events), pay special attention in aligning the events correctly with the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import find_events\n",
    "from mne.viz import plot_events\n",
    "\n",
    "## Find events.\n",
    "events = find_events(raw)\n",
    "print(events[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event list contains three columns. The first column corresponds to sample number. To convert this to seconds, you should divide the sample number by the used sampling frequency. The second column is reserved for the old value of the trigger channel at the time of transition, but is currently not in use. The third column is the trigger id (amplitude of the pulse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the events.\n",
    "event_id = {'Auditory/Left': 1, 'Auditory/Right': 2, 'Visual/Left': 3, 'Visual/Right': 4,\n",
    "            'smiley': 5, 'button': 32}\n",
    "color = {1: '#1f77b4', 2: '#ff7f0e', 3: '#2ca02c', 4: '#d62728', 5: '#9467bd', 32: '#8c564b'}\n",
    "\n",
    "fig = plot_events(events, raw.info['sfreq'], raw.first_samp, color=color, event_id=event_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Epoching\n",
    "\n",
    "Epoching describes the process of taking snapshots of the data centered around some event of interest. We will perform epoching using the `mne.Epochs` constructor. To do so, we need to define some parameters for our epoching.\n",
    "\n",
    "In this tutorial we are only interested in triggers 1, 2, 3 and 4. These triggers correspond to auditory and visual stimuli. The event_id here can be an int, a list of ints or a dict. With dicts it is possible to assign these ids to distinct categories.\n",
    "\n",
    "Next we need to define the windows of interest. The values tmin and tmax refer to offsets in relation to the events. Here we make epochs that collect the data from -500 ms before to 500 ms after the event. To get some meaningful results, we also want to baseline the epochs. Baselining computes the mean over the baseline period and adjusts the data accordingly. The epochs constructor uses a baseline period from tmin to 0.0 seconds by default, but it is wise to be explicit. That way you are less likely to end up with surprises along the way. None as the first element of the tuple refers to the start of the time window (-200 ms in this case). See `mne.Epochs` for more information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import Epochs, pick_types\n",
    "\n",
    "## Define events of interest.\n",
    "event_id = dict(LA=1, RA=2, LV=3, RV=4)\n",
    "\n",
    "## Define epoch lengths.\n",
    "tmin = -0.5\n",
    "tmax = 0.5\n",
    "baseline = (None, -0.1)\n",
    "\n",
    "## Define channels + rejection.\n",
    "picks = pick_types(raw.info, meg=False, eeg=True)\n",
    "reject = dict(eeg = 100e-6)\n",
    "\n",
    "## Perform epoching.\n",
    "epochs = Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=baseline,\n",
    "                picks=picks, reject=reject, verbose=False)\n",
    "\n",
    "## Drop bad epochs.\n",
    "epochs.drop_bad()\n",
    "\n",
    "fout = os.path.join('..','data','sub-01_task-audvis-epo.fif')\n",
    "epochs.save(fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: ERP Analysis & Visualization\n",
    "\n",
    "Now that we have defined our epochs, we can inspect the event-related potentials. There are a great many example tutorials on visualizing evoked potentials [here](https://www.martinos.org/mne/stable/auto_tutorials/plot_visualize_evoked.html). We demonstrate a few below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditory Stimuli\n",
    "First we compute the evoked response for each auditory stimulus condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average within each condition.\n",
    "LA_evoked = epochs['LA'].average()\n",
    "RA_evoked = epochs['RA'].average()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's plot the ERP for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Left auditory')\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "fig = LA_evoked.plot(spatial_colors=True, xlim=(-0.1,0.5), axes=ax);\n",
    "\n",
    "print('Right auditory')\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "fig = RA_evoked.plot(spatial_colors=True, xlim=(-0.1,0.5), axes=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the scalp topographic maps for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Left auditory')\n",
    "LA_evoked.plot_topomap(times=np.arange(0,0.35,0.05));\n",
    "print('Right auditory')\n",
    "RA_evoked.plot_topomap(times=np.arange(0,0.35,0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sensor layout from the previous notebook, we can choose a sensor that is clearly picking up a response to the auditory stimuli. **EEG 013** is a good candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.viz import plot_compare_evokeds\n",
    "\n",
    "## Construct evoked dictionary.\n",
    "auditory = evoked_dict = dict(LA=LA_evoked.copy(), RA=RA_evoked.copy())\n",
    "for k in ['LA', 'RA']: auditory[k] = auditory[k].crop(-0.1, 0.5)\n",
    "\n",
    "## Plot.\n",
    "fig = plot_compare_evokeds(auditory, picks=[epochs.ch_names.index('EEG 013')]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Stimuli\n",
    "First we compute the evoked response for each auditory stimulus condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average within each condition.\n",
    "LV_evoked = epochs['LV'].average()\n",
    "RV_evoked = epochs['RV'].average()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's plot the ERP for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Left visual')\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "fig = LV_evoked.plot(spatial_colors=True, xlim=(-0.1,0.5), axes=ax);\n",
    "\n",
    "print('Right visual')\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "fig = RV_evoked.plot(spatial_colors=True, xlim=(-0.1,0.5), axes=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the scalp topographic maps for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Left visual')\n",
    "LV_evoked.plot_topomap(times=np.arange(0.05,0.25,0.025));\n",
    "print('Right visual')\n",
    "RV_evoked.plot_topomap(times=np.arange(0.05,0.25,0.025));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sensor layout from the previous notebook, we can choose a sensor that is clearly picking up a response to the visual stimuli. Here we are observing strong laterality, so we visual two sensors: **EEG 056** and **EEG 057**. We will clearly see that that right- and left-presented stimuli are more strongly represented in the contralateral hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct evoked dictionary.\n",
    "visual = evoked_dict = dict(LV=LV_evoked, RV=RV_evoked)\n",
    "for k in ['LV', 'RV']: visual[k] = visual[k].crop(-0.1, 0.5)\n",
    "\n",
    "## Plot.\n",
    "plot_compare_evokeds(visual, picks=[epochs.ch_names.index('EEG 057')]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare_evokeds(visual, picks=[epochs.ch_names.index('EEG 056')]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
